{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#Data Ingestion\n",
    "## Data Ingestion--From the website we need to scrape the data\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x1fbce57c5b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://docs.smith.langchain.com/observability/tutorials/observability\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nAdd observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityTutorialsAdd observability to your LLM applicationOn this pageAdd observability to your LLM application\\nObservability is important for any software application, but especially so for LLM applications.\\nLLMs are non-deterministic by nature, meaning they can produce unexpected results.\\nThis makes them trickier than normal to debug.\\nLuckily, this is where LangSmith can help!\\nLangSmith has LLM-native observability, allowing you to get meaningful insights into your application.\\nNote that observability is important throughout all stages of application development - from prototyping, to beta testing, to production.\\nThere are different considerations at all stages, but they are all intricately tied together.\\nIn this tutorial we walk through the natural progression.\\nLet\\'s assume that we\\'re building a simple RAG application using the OpenAI SDK.\\nThe simple application we\\'re adding observability to looks like:\\nPythonTypeScriptfrom openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";const openAIClient = new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}\\nPrototyping\\u200b\\nHaving observability set up from the start can you help iterate much more quickly than you would otherwise be able to.\\nIt allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using.\\nIn this section we\\'ll walk through how to set up observability so you can have maximal observability as you are prototyping.\\nSet up your environment\\u200b\\nFirst, create an API key by navigating to the settings page.\\nNext, install the LangSmith SDK:\\nPython SDKTypeScript SDKpip install langsmithnpm install langsmith\\nFinally, set up the appropriate environment variables. This will log traces to the default project (though you can easily change that).\\nexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key>export LANGSMITH_PROJECT=default\\nnoteYou may see these variables referenced as LANGCHAIN_* in other places. These are all equivalent, however the best practice is to use LANGSMITH_TRACING, LANGSMITH_API_KEY, LANGSMITH_PROJECT.The LANGSMITH_PROJECT flag is only supported in JS SDK versions >= 0.2.16, use LANGCHAIN_PROJECT instead if you are using an older version.\\nTrace your LLM calls\\u200b\\nThe first thing you might want to trace is all your OpenAI calls.\\nAfter all, this is where the LLM is actually being called, so it is the most important part!\\nWe\\'ve tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper.\\nAll you have to do is modify your code to look something like:\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}\\nNotice how we import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client (openai_client = wrap_openai(OpenAI())).\\nWhat happens if you call it in the following way?\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the OpenAI call - it should look something like this\\n\\nTrace the whole chain\\u200b\\nGreat - we\\'ve traced the LLM call. But it\\'s often very informative to trace more than that.\\nLangSmith is built for tracing the entire LLM pipeline - so let\\'s do that!\\nWe can do this by modifying the code to now look something like this:\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });});\\nNotice how we import from langsmith import traceable and use it decorate the overall function (@traceable).\\nWhat happens if you call it in the following way?\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this\\n\\nTrace the retrieval step\\u200b\\nThere\\'s one last part of the application we haven\\'t traced - the retrieval step!\\nRetrieval is a key part of LLM applications, and we\\'ve made it easy to log retrieval steps as well.\\nAll we have to do is modify our code to look like:\\nPythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())@traceable(run_type=\"retriever\")def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());const retriever = traceable(  async function retriever(query: string) {    return [\"This is a document\"];  },  { run_type: \"retriever\" })const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });});\\nNotice how we import from langsmith import traceable and use it decorate the overall function (@traceable(run_type=\"retriever\")).\\nWhat happens if you call it in the following way?\\nrag(\"where did harrison work\")\\nThis will produce a trace of the whole chain including the retrieval step - it should look something like this\\n\\nBeta Testing\\u200b\\nThe next stage of LLM application development is beta testing your application.\\nThis is when you release it to a few initial users.\\nHaving good observability set up here is crucial as often you don\\'t know exactly how users will actually use your application, so this allows you get insights into how they do so.\\nThis also means that you probably want to make some changes to your tracing set up to better allow for that.\\nThis extends the observability you set up in the previous section\\nCollecting Feedback\\u200b\\nA huge part of having good observability during beta testing is collecting feedback.\\nWhat feedback you collect is often application specific - but at the very least a simple thumbs up/down is a good start.\\nAfter logging that feedback, you need to be able to easily associate it with the run that caused that.\\nLuckily LangSmith makes it easy to do that.\\nFirst, you need to log the feedback from your app.\\nAn easy way to do this is to keep track of a run ID for each run, and then use that to log feedback.\\nKeeping track of the run ID would look something like:\\nimport uuidrun_id = str(uuid.uuid4())rag(    \"where did harrison work\",    langsmith_extra={\"run_id\": run_id})\\nAssociating feedback with that run would look something like:\\nfrom langsmith import Clientls_client = Client()ls_client.create_feedback(    run_id,    key=\"user-score\",    score=1.0,)\\nOnce the feedback is logged, you can then see it associated with each run by clicking into the Metadata tab when inspecting the run.\\nIt should look something like this\\n\\nYou can also query for all runs with positive (or negative) feedback by using the filtering logic in the runs table.\\nYou can do this by creating a filter like the following:\\n\\nLogging Metadata\\u200b\\nIt is also a good idea to start logging metadata.\\nThis allows you to start keep track of different attributes of your app.\\nThis is important in allowing you to know what version or variant of your app was used to produce a given result.\\nFor this example, we will log the LLM used.\\nOftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering.\\nIn order to do that, we can add it as such:\\nfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())@traceable(run_type=\"retriever\")def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceable(metadata={\"llm\": \"gpt-4o-mini\"})def rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:    {docs}\"\"\".format(docs=\\'\\\\n\\'.join(docs))    return openai_client.chat.completions.create(messages = [        {\"role\": \"system\", \"content\": system_message},        {\"role\": \"user\", \"content\": question},    ], model=\"gpt-4o-mini\")\\nNotice we added @traceable(metadata={\"llm\": \"gpt-4o-mini\"}) to the rag function.\\nKeeping track of metadata in this way assumes that it is known ahead of time.\\nThis is fine for LLM types, but less desirable for other types of information - like a User ID.\\nIn order to log information that, we can pass it in at run time with the run ID.\\nimport uuidrun_id = str(uuid.uuid4())rag(    \"where did harrison work\",    langsmith_extra={\"run_id\": run_id, \"metadata\": {\"user_id\": \"harrison\"}})\\nNow that we\\'ve logged these two pieces of metadata, we should be able to see them both show up in the UI here.\\n\\nWe can filter for these pieces of information by constructing a filter like the following:\\n\\nProduction\\u200b\\nGreat - you\\'ve used this newfound observability to iterate quickly and gain confidence that your app is performing well.\\nTime to ship it to production!\\nWhat new observability do you need to add?\\nFirst of all, let\\'s note that the same observability you\\'ve already added will keep on providing value in production.\\nYou will continue to be able to drill down into particular runs.\\nIn production you likely have a LOT more traffic. So you don\\'t really want to be stuck looking at datapoints one at a time.\\nLuckily, LangSmith has a set of tools to help with observability in production.\\nMonitoring\\u200b\\nIf you click on the Monitor tab in a project, you will see a series of monitoring charts.\\nHere we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc.\\nYou can view these over time across a few different time bins.\\n\\nA/B Testing\\u200b\\nnoteGroup-by functionality for A/B testing requires at least 2 different values to exist for a given metadata key.\\nYou can also use this tab to perform a version of A/B Testing.\\nIn the previous tutorial we starting tracking a few different metadata attributes - one of which was llm.\\nWe can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts over time.\\nThis allows us to experiment with different LLMs (or prompts, or other) and track their performance over time.\\nIn order to do this, we just need to click on the Metadata button at the top.\\nThis will give us a drop down of options to choose from to group by:\\n\\nOnce we select this, we will start to see charts grouped by this attribute:\\n\\nDrilldown\\u200b\\nOne of the awesome abilities that LangSmith provides is the ability to easily drilldown into datapoints that you identify\\nas problematic while looking at monitoring charts.\\nIn order to do this, you can simply hover over a datapoint in the monitoring chart.\\nWhen you do this, you will be able to click the datapoint.\\nThis will lead you back to the runs table with a filtered view:\\n\\nConclusion\\u200b\\nIn this tutorial you saw how to set up your LLM application with best-in-class observability.\\nNo matter what stage your application is in, you will still benefit from observability.\\nIf you have more in-depth questions about observability, check out the how-to section for guides on topics like testing, prompt management, and more.Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability tutorialsNextObservability how-to guidesPrototypingSet up your environmentTrace your LLM callsTrace the whole chainTrace the retrieval stepBeta TestingCollecting FeedbackLogging MetadataProductionMonitoringA/B TestingDrilldownConclusionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data--> Docs-->Divide our Docuemnts into chunks dcouments-->text-->vectors-->Vector Embeddings--->Vector Store DB\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityQuick StartTutorialsAdd observability to your LLM applicationHow-to GuidesAnnotate code for tracingFilter traces in the applicationUpload files with tracesUse monitoring chartsDashboardsLog traces to specific projectSet up automation rulesOnline EvaluationSet a sampling rate for tracesAdd metadata and tags to tracesImplement distributed tracingAccess the current run (span) within a traced functionLog multimodal tracesLog retriever tracesLog custom LLM tracesPrevent logging of sensitive data in tracesQuery tracesShare or unshare a trace publiclyCompare tracesTrace generator functionsTrace with LangChain (Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='(Python and JS/TS)Trace with LangGraph (Python and JS/TS)Trace with Instructor (Python only)Trace with OpenTelemetryTrace with the Vercel AI SDK (JS/TS only)Trace without setting environment variablesTrace using the LangSmith REST APICalculate token-based costs for tracesTroubleshoot trace nesting[Beta] Bulk Exporting Trace DataHow to print detailed logs (Python SDK)Trace JS functions in serverless environmentsSet up threadsToubleshooting variable cachingSet up webhook notifications for rulesConceptual GuideEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceObservabilityTutorialsAdd observability to your LLM applicationOn this pageAdd observability to your LLM application'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"Observability is important for any software application, but especially so for LLM applications.\\nLLMs are non-deterministic by nature, meaning they can produce unexpected results.\\nThis makes them trickier than normal to debug.\\nLuckily, this is where LangSmith can help!\\nLangSmith has LLM-native observability, allowing you to get meaningful insights into your application.\\nNote that observability is important throughout all stages of application development - from prototyping, to beta testing, to production.\\nThere are different considerations at all stages, but they are all intricately tied together.\\nIn this tutorial we walk through the natural progression.\\nLet's assume that we're building a simple RAG application using the OpenAI SDK.\\nThe simple application we're adding observability to looks like:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIopenai_client = OpenAI()# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";const openAIClient = new OpenAI();// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"Prototyping\\u200b\\nHaving observability set up from the start can you help iterate much more quickly than you would otherwise be able to.\\nIt allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using.\\nIn this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.\\nSet up your environment\\u200b\\nFirst, create an API key by navigating to the settings page.\\nNext, install the LangSmith SDK:\\nPython SDKTypeScript SDKpip install langsmithnpm install langsmith\\nFinally, set up the appropriate environment variables. This will log traces to the default project (though you can easily change that).\\nexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key>export LANGSMITH_PROJECT=default\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"export LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key>export LANGSMITH_PROJECT=default\\nnoteYou may see these variables referenced as LANGCHAIN_* in other places. These are all equivalent, however the best practice is to use LANGSMITH_TRACING, LANGSMITH_API_KEY, LANGSMITH_PROJECT.The LANGSMITH_PROJECT flag is only supported in JS SDK versions >= 0.2.16, use LANGCHAIN_PROJECT instead if you are using an older version.\\nTrace your LLM calls\\u200b\\nThe first thing you might want to trace is all your OpenAI calls.\\nAfter all, this is where the LLM is actually being called, so it is the most important part!\\nWe've tried to make this as easy as possible with LangSmith by introducing a dead-simple OpenAI wrapper.\\nAll you have to do is modify your code to look something like:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())# This is the retriever we will use in RAG# This is mocked out, but it could be anything we wantdef retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results# This is the end-to-end RAG chain.# It does a retrieval step then calls OpenAIdef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='OpenAI } from \"openai\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());// This is the retriever we will use in RAG// This is mocked out, but it could be anything we wantasync function retriever(query: string) {  return [\"This is a document\"];}// This is the end-to-end RAG chain.// It does a retrieval step then calls OpenAIasync function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });}'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Notice how we import from langsmith.wrappers import wrap_openai and use it to wrap the OpenAI client (openai_client = wrap_openai(OpenAI())).\\nWhat happens if you call it in the following way?\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the OpenAI call - it should look something like this'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"Trace the whole chain\\u200b\\nGreat - we've traced the LLM call. But it's often very informative to trace more than that.\\nLangSmith is built for tracing the entire LLM pipeline - so let's do that!\\nWe can do this by modifying the code to now look something like this:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='openAIClient = wrapOpenAI(new OpenAI());async function retriever(query: string) {  return [\"This is a document\"];}const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });});'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Notice how we import from langsmith import traceable and use it decorate the overall function (@traceable).\\nWhat happens if you call it in the following way?\\nrag(\"where did harrison work\")\\nThis will produce a trace of just the entire pipeline (with the OpenAI call as a child run) - it should look something like this'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"Trace the retrieval step\\u200b\\nThere's one last part of the application we haven't traced - the retrieval step!\\nRetrieval is a key part of LLM applications, and we've made it easy to log retrieval steps as well.\\nAll we have to do is modify our code to look like:\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='PythonTypeScriptfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())@traceable(run_type=\"retriever\")def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceabledef rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:        {docs}\"\"\".format(docs=\"\\\\n\".join(docs))        return openai_client.chat.completions.create(        messages=[            {\"role\": \"system\", \"content\": system_message},            {\"role\": \"user\", \"content\": question},        ],        model=\"gpt-4o-mini\",    )import { OpenAI } from \"openai\";import { traceable } from \"langsmith/traceable\";import { wrapOpenAI } from \"langsmith/wrappers\";const openAIClient = wrapOpenAI(new OpenAI());const retriever = traceable(  async function retriever(query: string) {    return [\"This is a document\"];  },  { run_type: \"retriever\"'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='openAIClient = wrapOpenAI(new OpenAI());const retriever = traceable(  async function retriever(query: string) {    return [\"This is a document\"];  },  { run_type: \"retriever\" })const rag = traceable(async function rag(question: string) {  const docs = await retriever(question);    const systemMessage =    \"Answer the users question using only the provided information below:\\\\n\\\\n\" +    docs.join(\"\\\\n\");      return await openAIClient.chat.completions.create({    messages: [      { role: \"system\", content: systemMessage },      { role: \"user\", content: question },    ],    model: \"gpt-4o-mini\",  });});'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Notice how we import from langsmith import traceable and use it decorate the overall function (@traceable(run_type=\"retriever\")).\\nWhat happens if you call it in the following way?\\nrag(\"where did harrison work\")\\nThis will produce a trace of the whole chain including the retrieval step - it should look something like this'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"Beta Testing\\u200b\\nThe next stage of LLM application development is beta testing your application.\\nThis is when you release it to a few initial users.\\nHaving good observability set up here is crucial as often you don't know exactly how users will actually use your application, so this allows you get insights into how they do so.\\nThis also means that you probably want to make some changes to your tracing set up to better allow for that.\\nThis extends the observability you set up in the previous section\\nCollecting Feedback\\u200b\\nA huge part of having good observability during beta testing is collecting feedback.\\nWhat feedback you collect is often application specific - but at the very least a simple thumbs up/down is a good start.\\nAfter logging that feedback, you need to be able to easily associate it with the run that caused that.\\nLuckily LangSmith makes it easy to do that.\\nFirst, you need to log the feedback from your app.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='After logging that feedback, you need to be able to easily associate it with the run that caused that.\\nLuckily LangSmith makes it easy to do that.\\nFirst, you need to log the feedback from your app.\\nAn easy way to do this is to keep track of a run ID for each run, and then use that to log feedback.\\nKeeping track of the run ID would look something like:\\nimport uuidrun_id = str(uuid.uuid4())rag(    \"where did harrison work\",    langsmith_extra={\"run_id\": run_id})\\nAssociating feedback with that run would look something like:\\nfrom langsmith import Clientls_client = Client()ls_client.create_feedback(    run_id,    key=\"user-score\",    score=1.0,)\\nOnce the feedback is logged, you can then see it associated with each run by clicking into the Metadata tab when inspecting the run.\\nIt should look something like this'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='You can also query for all runs with positive (or negative) feedback by using the filtering logic in the runs table.\\nYou can do this by creating a filter like the following:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Logging Metadata\\u200b\\nIt is also a good idea to start logging metadata.\\nThis allows you to start keep track of different attributes of your app.\\nThis is important in allowing you to know what version or variant of your app was used to produce a given result.\\nFor this example, we will log the LLM used.\\nOftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering.\\nIn order to do that, we can add it as such:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Oftentimes you may be experimenting with different LLMs, so having that information as metadata can be useful for filtering.\\nIn order to do that, we can add it as such:\\nfrom openai import OpenAIfrom langsmith import traceablefrom langsmith.wrappers import wrap_openaiopenai_client = wrap_openai(OpenAI())@traceable(run_type=\"retriever\")def retriever(query: str):    results = [\"Harrison worked at Kensho\"]    return results@traceable(metadata={\"llm\": \"gpt-4o-mini\"})def rag(question):    docs = retriever(question)    system_message = \"\"\"Answer the users question using only the provided information below:    {docs}\"\"\".format(docs=\\'\\\\n\\'.join(docs))    return openai_client.chat.completions.create(messages = [        {\"role\": \"system\", \"content\": system_message},        {\"role\": \"user\", \"content\": question},    ], model=\"gpt-4o-mini\")\\nNotice we added @traceable(metadata={\"llm\": \"gpt-4o-mini\"}) to the rag function.\\nKeeping track of metadata in this way assumes that it is known ahead of time.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Notice we added @traceable(metadata={\"llm\": \"gpt-4o-mini\"}) to the rag function.\\nKeeping track of metadata in this way assumes that it is known ahead of time.\\nThis is fine for LLM types, but less desirable for other types of information - like a User ID.\\nIn order to log information that, we can pass it in at run time with the run ID.\\nimport uuidrun_id = str(uuid.uuid4())rag(    \"where did harrison work\",    langsmith_extra={\"run_id\": run_id, \"metadata\": {\"user_id\": \"harrison\"}})\\nNow that we\\'ve logged these two pieces of metadata, we should be able to see them both show up in the UI here.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"We can filter for these pieces of information by constructing a filter like the following:\\n\\nProduction\\u200b\\nGreat - you've used this newfound observability to iterate quickly and gain confidence that your app is performing well.\\nTime to ship it to production!\\nWhat new observability do you need to add?\\nFirst of all, let's note that the same observability you've already added will keep on providing value in production.\\nYou will continue to be able to drill down into particular runs.\\nIn production you likely have a LOT more traffic. So you don't really want to be stuck looking at datapoints one at a time.\\nLuckily, LangSmith has a set of tools to help with observability in production.\\nMonitoring\\u200b\\nIf you click on the Monitor tab in a project, you will see a series of monitoring charts.\\nHere we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc.\\nYou can view these over time across a few different time bins.\"),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='A/B Testing\\u200b\\nnoteGroup-by functionality for A/B testing requires at least 2 different values to exist for a given metadata key.\\nYou can also use this tab to perform a version of A/B Testing.\\nIn the previous tutorial we starting tracking a few different metadata attributes - one of which was llm.\\nWe can group the monitoring charts by ANY metadata attribute, and instantly get grouped charts over time.\\nThis allows us to experiment with different LLMs (or prompts, or other) and track their performance over time.\\nIn order to do this, we just need to click on the Metadata button at the top.\\nThis will give us a drop down of options to choose from to group by:\\n\\nOnce we select this, we will start to see charts grouped by this attribute:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Once we select this, we will start to see charts grouped by this attribute:\\n\\nDrilldown\\u200b\\nOne of the awesome abilities that LangSmith provides is the ability to easily drilldown into datapoints that you identify\\nas problematic while looking at monitoring charts.\\nIn order to do this, you can simply hover over a datapoint in the monitoring chart.\\nWhen you do this, you will be able to click the datapoint.\\nThis will lead you back to the runs table with a filtered view:'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Conclusion\\u200b\\nIn this tutorial you saw how to set up your LLM application with best-in-class observability.\\nNo matter what stage your application is in, you will still benefit from observability.\\nIf you have more in-depth questions about observability, check out the how-to section for guides on topics like testing, prompt management, and more.Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability tutorialsNextObservability how-to guidesPrototypingSet up your environmentTrace your LLM callsTrace the whole chainTrace the retrieval stepBeta TestingCollecting FeedbackLogging MetadataProductionMonitoringA/B TestingDrilldownConclusionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.faiss.FAISS object at 0x000001FBD9023F10>\n"
     ]
    }
   ],
   "source": [
    "print(vectorstoredb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Prototyping\\u200b\\nHaving observability set up from the start can you help iterate much more quickly than you would otherwise be able to.\\nIt allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using.\\nIn this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.\\nSet up your environment\\u200b\\nFirst, create an API key by navigating to the settings page.\\nNext, install the LangSmith SDK:\\nPython SDKTypeScript SDKpip install langsmithnpm install langsmith\\nFinally, set up the appropriate environment variables. This will log traces to the default project (though you can easily change that).\\nexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key>export LANGSMITH_PROJECT=default\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query From a vector db\n",
    "query=\"Having observability set up from the start can you help iterate much more quickly \"\n",
    "result=vectorstoredb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001FBD98C43D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001FBD98CB8B0>, root_client=<openai.OpenAI object at 0x000001FBD16D5450>, root_async_client=<openai.AsyncOpenAI object at 0x000001FBD98C53C0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval Chain, Document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided context emphasizes that having observability set up from the start helps you iterate much more quickly than you otherwise would be able to. It provides great visibility into your application as you rapidly iterate on the prompt, or change the data and models you are using. The context also mentions that there will be a walkthrough on how to set up observability to achieve maximal observability while prototyping.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "document_chain.invoke({\n",
    "    \"input\":\"Having observability set up from the start can you help iterate much more quickly \",\n",
    "    \"context\":[Document(page_content=\"Having observability set up from the start can you help iterate much more quickly than you would otherwise be able to. It allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using. In this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping. \")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1fbd9023f10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Input--->Retriever--->vectorstoredb\n",
    "\n",
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vectorstoredb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FBD9023F10>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001FBD98C43D0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001FBD98CB8B0>, root_client=<openai.OpenAI object at 0x000001FBD16D5450>, root_async_client=<openai.AsyncOpenAI object at 0x000001FBD98C53C0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the importance of observability in LLM applications?  \\n\\nObservability is crucial for LLM applications because LLMs are non-deterministic by nature and can produce unexpected results, making them more challenging to debug than typical software applications. Observability provides you with great visibility and meaningful insights into your application throughout all stages of development, from prototyping to production. It helps in quickly iterating during development and continuously monitoring performance once the application is live, allowing you to address issues efficiently and ensure the application performs well.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the response form the LLM\n",
    "response=retrieval_chain.invoke({\"input\":\"Having observability set up from the start can you help iterate much more quickly \"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Having observability set up from the start can you help iterate much more quickly ',\n",
       " 'context': [Document(id='59cd853d-14fc-4657-b95d-1d38e5f4ce9a', metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"Prototyping\\u200b\\nHaving observability set up from the start can you help iterate much more quickly than you would otherwise be able to.\\nIt allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using.\\nIn this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.\\nSet up your environment\\u200b\\nFirst, create an API key by navigating to the settings page.\\nNext, install the LangSmith SDK:\\nPython SDKTypeScript SDKpip install langsmithnpm install langsmith\\nFinally, set up the appropriate environment variables. This will log traces to the default project (though you can easily change that).\\nexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key>export LANGSMITH_PROJECT=default\"),\n",
       "  Document(id='73db8a7e-7c87-4ea3-802f-a3cc46b0b4d1', metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"We can filter for these pieces of information by constructing a filter like the following:\\n\\nProduction\\u200b\\nGreat - you've used this newfound observability to iterate quickly and gain confidence that your app is performing well.\\nTime to ship it to production!\\nWhat new observability do you need to add?\\nFirst of all, let's note that the same observability you've already added will keep on providing value in production.\\nYou will continue to be able to drill down into particular runs.\\nIn production you likely have a LOT more traffic. So you don't really want to be stuck looking at datapoints one at a time.\\nLuckily, LangSmith has a set of tools to help with observability in production.\\nMonitoring\\u200b\\nIf you click on the Monitor tab in a project, you will see a series of monitoring charts.\\nHere we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc.\\nYou can view these over time across a few different time bins.\"),\n",
       "  Document(id='daf4440e-3648-433d-acb7-d8dca44322c1', metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Conclusion\\u200b\\nIn this tutorial you saw how to set up your LLM application with best-in-class observability.\\nNo matter what stage your application is in, you will still benefit from observability.\\nIf you have more in-depth questions about observability, check out the how-to section for guides on topics like testing, prompt management, and more.Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability tutorialsNextObservability how-to guidesPrototypingSet up your environmentTrace your LLM callsTrace the whole chainTrace the retrieval stepBeta TestingCollecting FeedbackLogging MetadataProductionMonitoringA/B TestingDrilldownConclusionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'),\n",
       "  Document(id='3124eb5d-c151-457b-b3bf-7014fe3861c3', metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"Observability is important for any software application, but especially so for LLM applications.\\nLLMs are non-deterministic by nature, meaning they can produce unexpected results.\\nThis makes them trickier than normal to debug.\\nLuckily, this is where LangSmith can help!\\nLangSmith has LLM-native observability, allowing you to get meaningful insights into your application.\\nNote that observability is important throughout all stages of application development - from prototyping, to beta testing, to production.\\nThere are different considerations at all stages, but they are all intricately tied together.\\nIn this tutorial we walk through the natural progression.\\nLet's assume that we're building a simple RAG application using the OpenAI SDK.\\nThe simple application we're adding observability to looks like:\")],\n",
       " 'answer': 'What is the importance of observability in LLM applications?  \\n\\nObservability is crucial for LLM applications because LLMs are non-deterministic by nature and can produce unexpected results, making them more challenging to debug than typical software applications. Observability provides you with great visibility and meaningful insights into your application throughout all stages of development, from prototyping to production. It helps in quickly iterating during development and continuously monitoring performance once the application is live, allowing you to address issues efficiently and ensure the application performs well.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='59cd853d-14fc-4657-b95d-1d38e5f4ce9a', metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"Prototyping\\u200b\\nHaving observability set up from the start can you help iterate much more quickly than you would otherwise be able to.\\nIt allows you to have great visibility into your application as you are rapidly iterating on the prompt, or changing the data and models you are using.\\nIn this section we'll walk through how to set up observability so you can have maximal observability as you are prototyping.\\nSet up your environment\\u200b\\nFirst, create an API key by navigating to the settings page.\\nNext, install the LangSmith SDK:\\nPython SDKTypeScript SDKpip install langsmithnpm install langsmith\\nFinally, set up the appropriate environment variables. This will log traces to the default project (though you can easily change that).\\nexport LANGSMITH_TRACING=trueexport LANGSMITH_API_KEY=<your-api-key>export LANGSMITH_PROJECT=default\"),\n",
       " Document(id='73db8a7e-7c87-4ea3-802f-a3cc46b0b4d1', metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"We can filter for these pieces of information by constructing a filter like the following:\\n\\nProduction\\u200b\\nGreat - you've used this newfound observability to iterate quickly and gain confidence that your app is performing well.\\nTime to ship it to production!\\nWhat new observability do you need to add?\\nFirst of all, let's note that the same observability you've already added will keep on providing value in production.\\nYou will continue to be able to drill down into particular runs.\\nIn production you likely have a LOT more traffic. So you don't really want to be stuck looking at datapoints one at a time.\\nLuckily, LangSmith has a set of tools to help with observability in production.\\nMonitoring\\u200b\\nIf you click on the Monitor tab in a project, you will see a series of monitoring charts.\\nHere we track lots of LLM specific statistics - number of traces, feedback, time-to-first-token, etc.\\nYou can view these over time across a few different time bins.\"),\n",
       " Document(id='daf4440e-3648-433d-acb7-d8dca44322c1', metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content='Conclusion\\u200b\\nIn this tutorial you saw how to set up your LLM application with best-in-class observability.\\nNo matter what stage your application is in, you will still benefit from observability.\\nIf you have more in-depth questions about observability, check out the how-to section for guides on topics like testing, prompt management, and more.Was this page helpful?You can leave detailed feedback on GitHub.PreviousObservability tutorialsNextObservability how-to guidesPrototypingSet up your environmentTrace your LLM callsTrace the whole chainTrace the retrieval stepBeta TestingCollecting FeedbackLogging MetadataProductionMonitoringA/B TestingDrilldownConclusionCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2025 LangChain, Inc.'),\n",
       " Document(id='3124eb5d-c151-457b-b3bf-7014fe3861c3', metadata={'source': 'https://docs.smith.langchain.com/observability/tutorials/observability', 'title': 'Add observability to your LLM application | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Observability is important for any software application, but especially so for LLM applications.', 'language': 'en'}, page_content=\"Observability is important for any software application, but especially so for LLM applications.\\nLLMs are non-deterministic by nature, meaning they can produce unexpected results.\\nThis makes them trickier than normal to debug.\\nLuckily, this is where LangSmith can help!\\nLangSmith has LLM-native observability, allowing you to get meaningful insights into your application.\\nNote that observability is important throughout all stages of application development - from prototyping, to beta testing, to production.\\nThere are different considerations at all stages, but they are all intricately tied together.\\nIn this tutorial we walk through the natural progression.\\nLet's assume that we're building a simple RAG application using the OpenAI SDK.\\nThe simple application we're adding observability to looks like:\")]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response[\"context\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
